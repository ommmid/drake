{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Finding\n",
    "\n",
    "## Bisection\n",
    "<img src=\"./share/bisect.png\" width=400>\n",
    "\n",
    "## Secant\n",
    "<img src=\"./share/secant.png\" width=400>        \n",
    "    \n",
    "Two initial points ($x_0$ and $x_1$). Find their corresponding function values and connect those points. Cintinue with $x_1$ and $x_2$.     \n",
    "\n",
    "## Newton Raphson\n",
    "<img src=\"./share/newton_raph.jpg\" width=600>       \n",
    "\n",
    "For a system of equations, we have Jacobian here.\n",
    "\n",
    "<img src=\"./share/nr_system.png\" width=600>         \n",
    "\n",
    "## Optimization - Newton Raphson\n",
    "\n",
    "Now we can use Newton-Raphson to find the min of a function by finding the root of the derivative of the function:     \n",
    "Assume the function is esimated by Taylor series of the second order about point $x=a$:        \n",
    "$f(x)=f(a)+f^{'}(a)(x-a)+\\frac{1}{2}f^{''}(a)(x-a)^2$\n",
    "\n",
    "Take derivative of this, set equal to zero:     \n",
    "$\\frac{df}{dx} = f^{'}(a) + f^{''}(a)(x-a)=0$       \n",
    "$x = a -\\frac{f^{'}(a)}{f^{''}(a)}$     \n",
    "So, the iterative method is:\n",
    "$$x_{k+1} = x_{k} -\\frac{f^{'}(k)}{f^{''}(k)}$$                 \n",
    "\n",
    "For a multi-variable function, we have:         \n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - (\\nabla^2 f_(k))^{-1} \\nabla f(k) $$\n",
    "\n",
    "The second partial derivative $\\nabla^2 f$ is called Hessian Matrix.        \n",
    "\n",
    "<img src=\"./share/hessian.png\" width=400>\n",
    "\n",
    "Problem: if Hessian is not positive definite, then the method will not point in a descent direction.\n",
    "Gradient descent is more reliable but it might not converge at all.\n",
    "\n",
    "## Optimization - Gradient Descent\n",
    "$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\lambda \\nabla f(k)$           \n",
    "\n",
    "gradient descent moves forward toward the steepest slope at each step but Newton-Raphson is smarter and looks at the curvature too, so it is faster:        \n",
    "<p align=\"center\">\n",
    "    <img src=\"./share/gd_nr.png\" width=300>\n",
    "</p>\n",
    "\n",
    "\n",
    "## Optimization - Levenbergâ€“Marquardt\n",
    "They combined the Gradient descent with Newton-Raphson to get the best of both methods. Basically add the coefficeint of the function's gradient     \n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - (\\nabla^2 f_(k) + \\lambda \\mathbf{I})^{-1} \\nabla f(k) $$     \n",
    "\n",
    "The update algorithm:       \n",
    "If $f_(\\mathbf{x}_{k+1}) < f(\\mathbf{x}_{k})$, it means we are in the right direction, so give more power to the curvature part, meaning reduce $\\lambda$. On the other hand, if the cost function goes up, retract the step and increase $\\lambda$ by a factor of 10 to give more power to gradient descent.       \n",
    "\n",
    "So overall, it is faster than gradient descent and more reliable thatn Newton-Raphson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
